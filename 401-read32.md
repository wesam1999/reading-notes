# 401-read32.md
## Pros and Cons of Serverless Architectures
A serverless architecture is a way to build and run applications and services without having to manage their infrastructure.<br />

Just as you become experienced in using certain tools and architectures, something new is introduced and you’re left asking yourself if you now have to start all over again.<br />
It’s a tough predicament that could throw you off as it can lead to large scale restructuring and code refactoring.<br />

What if you have to finish a current migration only to do it all over again in a few years and with a new product? <br />
Have you found yourself in that scenario? It’s no easy feat, and requires research and consideration.<br />

At the end of the day, if you are able to deliver a great service to your customers, it probably won’t make much sense to consider such a significant change.<br />
In contrast, refusing to embrace innovation could keep you in the past — and there are times when technology drastically leaps forward to a better and brighter future..<br />

And this is a bit of what goes through our brain when we start seeing terms like “serverless architecture” everywhere we look.<br />

If the thought of change gives you a headache, I suggest you consider this statement: when it comes to web architecture, composability is paramount.<br />

Do you agree? If so, then serverless is the architecture you’ve been waiting for. If not, that’s okay, because either way we’re going to find out.<br />

Personally I believe this to be true.<br />
When I think about how all my frontend applications are structured, and the success many organizations have enjoyed by moving away from a traditional Monolithic Architecture pattern to a decoupled Microservice Architecture pattern, <br />
I see a clear roadmap leading straight for Serverless Architecture. <br />
I hope by the end of this post you will see it too. Let’s go through what serverless architecture is and analyze the good, the bad and the ugly.<br />

What is a serverless architecture?<br />
To begin, let’s get a clear understanding about what serverless architecture is not.<br />

Serverless architecture is not a replacement for microservices nor does it mean there is no server.<br />
In fact, cloud service providers will take care of the server infrastructure for you and microservices architecture and serverless computing can work together, or be used separately, even within the same application.<br />

It’s very easy to blur the lines between the various cloud services making it difficult to determine exactly what constitutes serverless architecture.<br />

Many believe serverless to just be Function-as-a-Service, or FaaS, and in its simplest form this is certainly correct.<br />

FaaS is a subset of serverless, otherwise known as serverless functions. These functions are triggered by an event, such as when a user clicks a button. <br />
Cloud service providers will take care of the infrastructure these functions run on so literally all you have to do is write some code and deploy. <br />
Communication between your frontend and serverless function is as simple as an API call.<br />

Serverless computing cloud services were first introduced by Amazon Web Services in 2014 with AWS Lambda: this is Amazon’s FaaS offering. Other popular cloud vendor FaaS include:<br />

Netlify Functions<br />
Vercel Serverless Functions<br />
Cloudflare Workers<br />
Google Cloud Functions<br />
Microsoft Azure Functions<br />
IBM Cloud Functions<br />
Cloud vendors provide many other services that tend to be conflated with serverless:<br />

Infrastructure-as-a-Service (IaaS),<br />
Platform-as-a-Service (PaaS),<br />
Software-as-a-Service (SaaS),<br />
and Backend-as-a-Service (BaaS).<br />
Now I’m not going to dive into detail about these technologies because they are each a topic on their own and this is more of a high-level overview of serverless. <br />
Instead I will simply say that the common characteristic of all of them is that cloud service providers take care of all the infrastructure elements of these services so that you don’t have to.<br />

That means you save on time, resources, complexity and money while shifting your focus solely on your application and customer experience.<br />

This was originally what the “A” in Jam(stack) stood for and is a fundamental advantage of the MACH (Microservices, API-first, Cloud-native, Headless) ecosystem of which Serverless Architecture is often a key component.<br />
Why should you use a serverless architecture?
Serverless and associated cloud services are still relatively new and we are seeing incredible advancements with new technologies released into the market each year.<br />

However, despite offering many advantages over traditional architectures, serverless computing is not a silver bullet.<br />

As with anything, there are reasons for why serverless architecture may not meet your requirements. Let’s take a look.<br />

Pros of Serverless Architecture<br />
No server management. Serverless computing runs on servers, but those servers are managed by cloud service providers. <br />
Zero server management doesn’t limit you in any way either, just the opposite in fact thanks to scaling options, optimal availability and the elimination of idle capacity to name a few.<br />
Cost.
There are a number of ways in which serverless helps to save money. With traditional server architecture, you would have to predict and purchase server capacity, usually more than you need, to ensure your application does not face any performance bottlenecks or downtime.<br />
With serverless, cloud service providers only charge you for what you use since your code will only run once triggered by an event when a backend service is needed. <br />
I don’t know about you, but that sounds exactly right to me.
Additionally, since the overhead and maintenance of servers is taken care of by your provider, you won’t be paying developers and other IT professionals to spend time on any of it.<br /><br />
Just as cloud computing saves a fortune in hardware costs, serverless saves a fortune in human resource costs as well.<br />

Scalability.
Applications built on serverless architecture can be scaled endlessly and automatically.<br />
No worrying about a spike in traffic bringing down your site or causing poor performance as you may experience with a fixed server capacity.<br />
Costs will go up of course as your user base or usage increases, but as a previous employer of mine likes to say, that’s a nice problem to have.<br />

Security.
You may find that many articles on serverless architecture lists security as a disadvantage. <br />
This post will mention some of the most commonly shared security concerns in the next section,<br />
but it’s important to understand that top cloud vendors are dedicating themselves to providing the most secure, performant and available service possible.<br />
That is a key component of their business model so it stands to reason that they are employing some of the best in the business to create and maintain these services, not to mention ensuring they provide the absolute best practices.<br />
There is some security related to the application itself that developers still need to consider, but the bulk of it is handled for you by industry experts and in my opinion this is a major advantage.<br />

Quicker time to market.
Development environments are easier to set up and not having to manage servers leads to accelerated delivery and rapid deployment.<br />
This is especially critical for a Minimum Viable Product with the added bonus that everything is decoupled meaning you can add or remove services at will without the huge amount of code changes that a monolith application would require.<br />

Reduced latency.
Thanks to Content Delivery Networks (CDN) and Edge Networks, serverless functions can be run on servers that are closer to end users all over the globe.<br />
Popular examples of edge computing providers for the Jamstack include:<br />

Cloudflare Workers,
AWS Lambda@Edge,
Netlify Edge,
and most recently Vercel Edge Functions.
Cons of Serverless Architecture
Vendor lock-in. While it is certainly possible to pick and choose services from different vendors, the easiest way to go will be to use a single cloud services provider, such as AWS, since each vendor will have their own way of doing things.<br />
This can be challenging if you ever want to migrate to a different provider and you will be completely reliant on vendors to provide an optimal service at all times. If there is an infrastructure problem of any kind, you have to wait for them to fix it.<br />

Security. Cloud providers will often run code from several customers on the same server at any given time.<br />
They do this by using a technique known as Multitenancy. To put it plainly, customers are tenants that only have access to their share of the server.<br />
This presents a possible scenario where data is exposed due to misconfiguration of that server.<br />

Performance impact. Serverless computing is not constantly running.<br />
When a function is called for the first time, it requires a “cold start” which is to say that a container needs to spin up before the function can be run.<br />
This may degrade performance although it’s important to note that a container will continue to run for a period of time after the API call is complete in case it is needed again soon after at which point we get a “warm start” without the added latency. <br />
Thanks to edge computing, cold starts are becoming less and less of an issue and this will only improve over time.<br />

Debugging and testing. Debugging is complex due to reduced visibility of backend processes that are managed by the cloud provider. <br />
Additionally, a serverless environment can be difficult to replicate in order to perform integration tests.<br /> 
It’s not all bad news though. As the serverless ecosystem continues to grow, new platforms and services are released into the market to solve issues such as this.<br />
One possible solution would be to use Datadog’s End-to-end Serverless Monitoring.<br />

Final thoughts<br />
There are many use cases for serverless architecture.<br />
Most revolve around low-computing operations with unpredictable traffic or used in conjunction with microservices via REST or GraphQL APIs.<br />

There is no question that migrating from a legacy infrastructure to serverless could be very challenging, especially if it requires a complete rethink of how your application is structured, but the beauty of switching to serverless is you can do it a piece at a time.<br />

Maybe this relatively new architecture doesn’t fulfill all your needs right now, but investing your time in serverless will pay dividends in the end as we have only just scratched the surface of its capabilities.<br />

Lastly, since Jamstack sites and applications are heavily focused on the frontend, serverless is the perfect method of integrating backend functionality.<br />

Lack of experience with both of these architectures can certainly give a company pause, but we’re here to answer any questions you may have and ensure a smooth transition.

## AWS Amplify Kool-Aid

All about AWS Amplify<br />
AWS is a full-stack platform designed to assist web and mobile developers in developing full-stack and scalable applications that are hosted by AWS. <br />
The platform includes a plethora of tools and services that enable users to easily configure backends, connect apps, deploy static web apps instantly, and manage content outside of the AWS console.<br />

AWS Amplify, which was launched in 2017, is a full-suite package of tools and services designed to help developers easily create and launch apps. <br />
It may also include code libraries, ready-to-use components, and a built-in command-line interface (CLI). 
The most significant advantage of this tool is that it allows you to quickly and securely integrate a wide range of functions ranging from API to AI.<br />

Likewise, the user experience is another reason for the launch of AWS Amplify.<br />
The most important aspect that must be considered when developing any application is user experience.<br />
AWS Amplify was designed to unify the user experience across multiple platforms, including web and mobile.<br />

It gives users the freedom to build on the platform with which they are most comfortable, which is especially useful for front-end development. Most Amplify users also claim that its scalability factor makes full-stack development much more comfortable.<br />

Now, let’s get familiar with the working mechanism of AWS Amplify.<br />

![](https://miro.medium.com/max/628/0*aMhlZ82pkQ7HxzlZ.png)
How does AWS Amplify Work?
Here, considering AWS Amplify to be a JavaScript library that allows you to create and deploy serverless applications in the cloud.<br />
It is a full-stack application platform with both client-side and server-side code. In a nutshell, AWS Amplify is made up of three major parts:<br />

1. Libraries<br />

2. User Interface (UI)<br />

3. Command Line Interface (CLI) Toolchain<br />

Now, all of these elements work together to manage the application development lifecycle. Here’s a quick rundown of each component:<br />

1. Libraries<br />

Using AWS Amplify, you can add, integrate and interact with AWS cloud services through this component. <br />
The library also facilitates safe authentication, storage of files, data stocking, serverless APIs, analytics, push notifications, AR/VR, and multiple other applications’ features.<br />

2. User Interface (UI)<br />

Here, in AWS Amplify, the pre-built UI components, including the authentication-requirement component, are developed around cloud workflows in your application.<br />

3. Command Line Interface (CLI) Toolchain<br />

The component Command Line Interface (CLI) Toolchain helps your application to be scaled. Also, if you ever need to add additional cloud services and functions, CLI commands can change your AWS-managed backends efficiently.<br />

Now, let us find out the main advantages of AWS Amplify in the next section.<br />
![](https://miro.medium.com/max/628/0*IPU-u98CdtJ3q-H2.png)
Advantages of AWS Amplify<br />
1. Free to Start<br />
2.Development is Easy and UI driven<br />
3. Backend Support<br />
4. Web-Based Analytics<br />
5. Usage-Based Payment<br />

Disadvantages of AWS Amplify<br />
1. Consistent Changes<br />
2. Cost<br />
3. Traffic Distribution<br />
4.  Higher Learning Curve<br />

## GraphQL Intro (just read the first few paragraphs, up until the query code)
Data Modeling<br />
The schema is the basis for your GraphQL API. You define the schema of your data model using the GraphQL Schema Definition Language (SDL) and Slicknode then uses the data model to add the CRUD mutations, filters, sorting, pagination, etc. to your API.<br />

Instructions<br />
We will start with a simple version of the blog schema that we extend throughout the tutorial. To determine the schema for your application, we just have to convert the requirements into a GraphQL schema.<br />

If you are using VS Code, you can install the Slicknode VS Code extension with snippets and other features from the marketplace.<br />

The schema of our blog is part of the blog module and should therefore be added to the schema.graphql file in that module folder.<br />
Open the file in your favorite IDE and add the following:<br />

modules/blog/schema.graphql:<br />

```
type Blog_Article implements Node & Content {
  id: ID!
  title: String!
  slug: String! @unique
  mainImage: Image
  text: String! @input(type: MARKDOWN)
  category: Blog_Category!

  # `Content` interface fields for advanced content managment functionality
  contentNode: ContentNode!
  locale: Locale!
  status: ContentStatus!
  publishedAt: DateTime
  publishedBy: User
  createdAt: DateTime!
  createdBy: User
  lastUpdatedAt: DateTime
  lastUpdatedBy: User
}

type Blog_Category implements Node {
  id: ID!
  name: String!
  slug: String! @unique
}
```
Validation<br />
Check the validity of your schema and configuration by running the status command and fix any errors it might display:<br />


slicknode status<br />
This validates the schema, checks for syntax errors, naming conventions and consistency.<br />
In case it does not detect any errors in your schema definition, it will compare the changes to the schema that is currently deployed to the cloud and will output a list of all the pending changes.<br />

Deployment<br />
Deploy the schema changes to the Slicknode Cloud by running:<br />


slicknode deploy<br />
This will show you a preview of the pending changes that will be applied after confirmation.<br />

Now your Slicknode GraphQL API has blog functionality and is ready to be used. We will add content and use the API in the next step.<br />

Explanations<br />
If we look at this schema, we can see how types are defined with the GraphQL SDL:<br />

Naming convention<br />
It always starts with the keyword type followed by the typename. Note that there is always the namespace of the module prepended to the type name (Blog_). <br />
This is a Slicknode convention to avoid name collisions of types from different modules. <br />
The last part of the types should always use CamelCase. There can be any number of type definitions in one schema file.<br />

Node Interface<br />
The definition implements Node means that the type implements the Node interface. Interfaces are a special type in GraphQL that let you define a set of fields. <br />
(Learn more about interfaces) Every type that implements the interface has to implement all the fields of that interface. <br />
The Node interface is defined by the Relay standard and has just the field id with the scalar type ID!. <br />
The exclamation mark behind type names indicates that this field value is required and cannot be NULL. <br />
Slicknode uses the Node interface to determine for which types it should generate database tables and CRUD mutations / fields.<br /><br />
So for most types you would implement the Node interface and add the corresponding field id: ID!.<br />

Input Directive<br />
Slicknode has a special @input directive which allows you to configure the appearance of the input element in the data browser. <br />
By default, fields of type String have a simple text input element. <br />
To change this to a markdown editor, you can configure the input element by adding the input directive after the field:<br />
```

type Blog_Article implements Node & Content {
  text: String! @input(type: MARKDOWN)
}
```
There are several input elements available, check the documentation for the type to see which input elements are available.<br />

Content Interface<br />
The Content is an interface that is part of the content module of Slicknode. <br />
It automatically adds additional functionality to our Blog_Article type like a version history, localiztion, <br />
a publishing workflow and it automatically updates the fields createdBy, lastUpdatedBy and publishedBy whenever a user makes changes to an article. All we have to do is implement the Content interface in the type and add the fields:<br />

```
type Blog_Article implements Node & Content {
  id: ID!

  # ...

  # `Content` interface fields for advanced content managment functionality
  contentNode: ContentNode!
  locale: Locale!
  status: ContentStatus!
  publishedAt: DateTime
  publishedBy: User
  createdAt: DateTime!
  createdBy: User
  lastUpdatedAt: DateTime
  lastUpdatedBy: User
}
```
When you add multiple interfaces to a type, the interface names have to be separated by &.<br />

To learn more about the Content, click here.<br />

String fields<br />
The fields slug, title and text are defined with the data type String!. We add the exclamation mark after the type name to make sure that these values cannot be NULL.<br />

For the field slug we also add a @unique directive. Directives can be added to elements in your GraphQL documents to provide additional instructions for the processing GraphQL engine. <br /><br />
Slicknode supports a variety of directives that we will get to know in the tutorial. <br />
The @unique directive ensures that the value is unique across all stored nodes of that type.<br />
We will use the slug to build our unique SEO friendly URLs and need to have a unique identifier.<br />

User relations
The Blog_Article type has several fields of type User (publishedBy, createdBy, lastUpdatedBy).<br />
The User type is a special type that is provided by the auth core module.<br />
It is used for authentication, authorization and has some builtin functionality, for example to store encrypted password hashes etc.<br />

Those are the first relations. 
One user can create multiple articles, but each article can only have one user that created the article (One-to-many relationship). <br />
Adding a field with another node type creates a column in the database that references the related user object with a foreign key constraint, which ensures that you can only add existing users as an author and not arbitrary user IDs.<br />
The fields of type User do not have an exclamation marks at the end and are therefore optional.<br />

However, when you create your data model, you should always plan for the full lifecycle of all related objects: What happens when you delete a user object that has published articles? <br />
In that case this would violate the foreign key constraint.
Slicknode enforces foreign key constraints at the database level, it would automatically delete all related objects if this were a required field to keep the data consistent. (Cascading delete)<br />

This is why we define this field as not required: If we delete a user from the database, we can still keep the articles of that user.<br />
The field author would just be set to NULL and we can display in our frontend something like "Deleted User".<br />

Category<br />
For the category, we define a separate type that is also persisted to the database (implements Node) so that we can create relations to the articles.<br />
The relation between the Blog_Category and the Blog_Article can be created the same way as for the author field.<br />
In this case, we define the field category with a type of Blog_Category!. This means, that the category can never be NULL.<br />
If we delete a category, it would also automatically delete all articles in that category. Let's assume that this is the business logic we want for our blog.****<br />












