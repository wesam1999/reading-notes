# 401-read30.md
## Introduction to Hash Tables

**Hash Table**
A Hash Table is a data structure that stores data in an associative manner. <br />
It is made up of two parts: an array, where the data is stored, and a Hash Function which is a mapping function.<br />
Basically, a Hash Function is a function that takes things from one space and maps them to a space for indexing.<br />

A Hash Table is used to implement structures such as dictionary, map, or associative array and it is a data structure in which insertion and search operations are very fast.<br />
<br />
The idea behind a Hash Table is that for each element we want to store, we calculate a unique address and we put the value at this index in the array. <br /><br />
When we need to find a value, we once again calculate its index and then return the value. In other words, a Hash Table allows us to store and retrieve objects by key.<br />
<br />
A Hash Table is an array and initially, this array is empty.<br />
When we want to put a value into the Hash Table under a certain key, that key will be used to compute an index in the array. We can see it like so:<br />
```
hashTable["firstName"] = "John"
    hashTable array:
    +--------------+
    | 0:           |
    +--------------+
    | 1:           |
    +--------------+
    | 2: firstName | ---> John
    +--------------+
    | 3:           |
    +--------------+
    | 4:           |
    +--------------+
```
Here, the key is “firstName” and it maps to index <br />
2. It means that our Hash Table asked for the index of the key “firstName” and the Hash Function returned<br /> 
2. We can see it like so:<br />
```
key ---> HASH FUNCTION --> array_index
```
**Hash Function**
Hashing is a technique that lets us convert a range of key values into a range of indexes of an array. <br />
The values returned by a Hash Function are called Hash Values. 
There are multiple ways for constructing a Hash Function but it is important that the function has some basic requirements:<br />

The Hash Value is fully determined by the data being hashed and it should be easy to compute<br />
The Hash Function uses all the input data<br />
The Hash Function should provide a uniform distribution across the Hash Table<br />
The Hash Function generates very different Hash Values for similar strings<br />
Solving collisions<br />
Let’s say that our Hash Function will compute the index of a specific string by summing the ASCII values of the characters modulo 373 (that is a prime number). <br />
Our set of string will be {“abcdef”, “bcdefa”, “cdefab” , “defabc”}. 
Knowing that the ASCII values of a, b, c, d, e, and f are 97, 98, 99, 100, 101, and 102, our addition will always give us the same result (597) just like the operation 597%373.<br />

When different elements get the same Hash Value, it is a called a collision.<br />
So, how can we handle such a situation? The two most popular ways to solve this are called Chaining and Open Addressing.<br />

**Chaining**
When we use this method, we just use a Linked List and put all the values with the same Hash Value into it and so the Hash Table becomes an array of Linked Lists.<br />

**Open Addressing**
Also called Linear Probing, when we use this method and we want to add a new entry, the Hash Index of the Hashed Value is computed and then the array is examined.<br />
If the slot is empty, then the value is inserted. If not, we calculate another Hash Value and check that slot and we continue until a place for the value is found.<br />
The probe sequence will be like so:<br />
```
index = index % hashTableSize
index = (index + 1) % hashTableSize
index = (index + 2) % hashTableSize
index = (index + 3) % hashTableSize
...
```
**Quadratic Probing**
Quadratic Probing is similar to Linear Probing but the difference is the interval between successive probes.<br />
With Quadratic Probing, the sequence would be something like this:<br />
```
index = index % hashTableSize
index = (index + 1^2) % hashTableSize
index = (index + 2^2) % hashTableSize
index = (index + 3^2) % hashTableSize
...
```
**Double Hashing**
Double Hashing works on a similar idea to Linear and Quadratic Probing.<br />
The difference is that a second Hash Function is used to determine the location of the next empty slot. The sequence would look like this:<br />
```
index = (h1(key) + 1 * h2(key)) % hashTableSize;
index = (h1(key) + 2 * h2(key)) % hashTableSize;
index = (h1(key) + 3 * h2(key)) % hashTableSize;
...
```
Basic Examples<br />
Knowing what we know, it is time to make really simple examples using Java.<br />

**Linear Probing**
First, let’s define a class for the Entry:<br />
```
public class HashEntry {
    private int key;
    private int value;
    HashEntry(int key, int value) {
            this.key = key;
            this.value = value;
    }
    public int getKey() {
            return key;
    }
    public int getValue() {
            return value;
    }
}
```
And now, let’s create our Hash Table:
```
public class HashTable {
    private final static int TABLE_SIZE = 64;
    HashEntry[] table;
    HashTable() {
        table = new HashEntry[TABLE_SIZE];
        for (int i = 0; i < TABLE_SIZE; i++) {
            table[i] = null;
        }
    }
    public int get(int key) {
        int hash = (key % TABLE_SIZE);
        while (table[hash] != null && table[hash].getKey() != key) {
            hash = (hash + 1) % TABLE_SIZE;
        }
        if (table[hash] == null) {
            return -1;
        } else {
            return table[hash].getValue();
        }
    }
    public void put(int key, int value) {
        int hash = (key % TABLE_SIZE);
        while (table[hash] != null && table[hash].getKey() != key) {
            hash = (hash + 1) % TABLE_SIZE;
        }
        table[hash] = new HashEntry(key, value);
    }
}
```
**Chaining**
First, we define the Nodes of our Linked List.<br />
```
public class HashNode {
    private int key;
    private int value;
    private HashNode next;
    HashNode(int key, int value) {
        this.key = key;
        this.value = value;
        this.next = null;
    }
    public int getValue() {
        return value;
    }
    public void setValue(int value) {
        this.value = value;
    }
    public int getKey() {
        return key;
    }
    public HashNode getNext() {
        return next;
    }
    public void setNext(HashNode next) {
        this.next = next;
    }
}
```
Now, let’s create our Hash Table:
```
public class HashTable {
    private final static int TABLE_SIZE = 64;
    HashNode[] table;
    HashTable() {
        table = new HashNode[TABLE_SIZE];
        for (int i = 0; i < TABLE_SIZE; i++) {
            table[i] = null;
        }
    }
    public int get(int key) {
        int hash = (key % TABLE_SIZE);
        if (table[hash] == null) {
            return -1;
        } else {
            HashNode entry = table[hash];
            while (entry != null && entry.getKey() != key) {
                entry = entry.getNext();
            }
            if (entry == null) {
                return -1;
            } else {
                return entry.getValue();
            }
        }
    }
    public void put(int key, int value) {
        int hash = (key % TABLE_SIZE);
        if (table[hash] == null) {
            table[hash] = new HashNode(key, value);
        } else {
            HashNode entry = table[hash];
            while (entry.getNext() != null && entry.getKey() != key) {
                entry = entry.getNext();
            }
            if (entry.getKey() == key) {
                entry.setValue(value);
            } else {
                entry.setNext(new HashNode(key, value));
            }
        }
    }
}

```

## basics of hash tables
**Basics**
Hash tables are used to implement map and set data structures in most common programming languages. In C++ and Java they are part of the standard libraries, while Python and Go have builtin dictionaries and maps.<br />

A hash table is an unordered collection of key-value pairs, where each key is unique.<br />

Hash tables offer a combination of efficient lookup, insert and delete operations. Neither arrays nor linked lists can achieve this:<br />

a lookup in an unsorted array takes linear worst-case time;<br />
in a sorted array, a lookup using binary search is very fast, but insertions become inefficient;<br />
in a linked list an insertion can be efficient, but lookups take linear time.<br />
**Hashing with chaining (simplified example)**
The most common hash table implementation uses chaining with linked lists to resolve collisions. This combines the best properties of arrays and linked lists.<br />

Hash table operations are performed in two steps:<br />

A key is converted into an integer index by using a hash function.<br />
This index decides the linked list where the key-value pair record belongs.<br />


![](https://yourbasic.org/algorithms/hash-table.png)
This hash table consists of an array with 1000 entries, each of which refers to a linked lists of key-value pairs.<br />
Let’s start with a somewhat simplified example: a data structure that can store up to 1000 records with random integer keys.<br />

To distribute the data evenly, we use several short lists. <br />
All records with keys that end with 000 belong to one list, those with keys that end with 001 belong to another one, and so on. <br />
There is a total of 1000 such lists. This structure can be represented as an array of lists:<br />
```
var table = new LinkedList
```
where LinkedList denotes a linked list of key-value pairs.<br />

Inserting a new record (key, value) is a two-step procedure:<br />

we extract the three last digits of the key, hash = key % 1000,<br />
and then insert the key and its value into the list located at table[hash].<br />
```
hash = key % 1000
table[hash].AddFirst(key, value)
```
This is a constant time operation.<br />

A lookup is implemented by<br />
```
value = table[key%1000].Find(key)
```
Since the keys are random, there will be roughly the same number of records in each list.<br />
Since there are 1000 lists and at most 1000 records, there will likely be very few records in the list table[key%1000] and therefore the lookup operation will be fast.<br />

The average time complexity of both the lookup and insert operations is O(1).<br />
Using the same technique, deletion can also be implemented in constant average time.<br />

**Realistic hash function example**
We want to generalize this basic idea to more complicated keys that aren’t evenly distributed.<br />
The number of records in each list must remain small, and the records must be evenly distributed over the lists. <br />
To achieve this we just need to change the hash function, the function which selects the list where a key belongs.<br />

The hash function in the example above is hash = key % 1000. It takes a key (a positive integer) as input and produces a number in the interval 0..999.<br />

In general, a hash function is a function from E to 0..size-1, where E is the set of all possible keys, and size is the number of entry points in the hash table.<br />
We want this function to be uniform: it should map the expected inputs as evenly as possible over its output range.<br />

Java’s implementation of hash functions for strings is a good example. The hashCode method in the String class computes the value<br />

s[0]·31n-1 + s[1]·31n-2 + … + s[n-1]<br />

using int arithmetic, where s[i] is the i:th character of the string, and n is the length of the string.<br />

This method can be used as a hash function like this:<br />
```
hash = Math.abs(s.hashCode() % size)
```
where size is the number of entry points in the hash table.<br />

Note that this function<br />

depends on all characters in the string,<br />
and that the value changes when we change the order of the characters.<br />
Two properties that should hold for a good hash function.<br />

**Resizing in constant amortized time**
The efficiency of a hash table depends on the fact that the table size is proportional to the number of records.<br />
If the number of records is not known in advance, the table must be resized when the lists become too long:<br />

a new larger table is allocated,<br />
each record is removed from the old table,<br />
and inserted into the new table.<br />
If the table size is increased by a constant factor for each resizing, i.e. by doubling its size, this strategy gives amortized constant time performance for insertions.<br />




















